{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤— Transformers provider a Trainer API to easily train or fine-tune Transformer models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div dir=\"auto\">Ø¨Ø§ Tainer Ø¯Ø± Transformers Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯.</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### keywords: Train a transformer model, Train BERT model, Ø¢Ù…ÙˆØ²Ø´ Bert, Ø¢Ù…ÙˆØ²Ø´ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Here is how we can easily preprocess the GLUE MRPC dataset using dynamic padding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"auto\">1. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…Ø§ Ù†Ø­ÙˆÙ‡ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ GLUE MRPC Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² padding Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ© Ø¢ÙˆØ±Ø¯Ù‡â€ŒØ§ÛŒÙ….</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def fnTokenizer(x):\n",
    "    return tokenizer(x[\"sentence1\"], x[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(fnTokenizer, batched = True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. We also need a model and some training arguments before creating the Trainer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"auto\">2. Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…Ø§ Ø¨Ù‡ ÛŒÚ© Ù…Ø¯Ù„ Ùˆ Ú†Ù†Ø¯ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† (Ú©Ø§Ù†ÙÛŒÚ¯) Ø¢Ù…ÙˆØ²Ø´ Ù‚Ø¨Ù„ Ø§Ø² Ø§ÛŒØ¬Ø§Ø¯ Trainer Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ….</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. We can then pass everything to the Trainer class and start training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"auto\">3. Ø³Ù¾Ø³ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ù‡Ø± Ø¢Ù†Ú†Ù‡ Ú©Ù‡ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ø±Ø§ Ø¨Ù‡ Ú©Ù„Ø§Ø³ Trainer Ù¾Ø§Ø³ Ø¨Ø¯Ù‡ÛŒÙ… Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ø±Ø§ Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒÙ….</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. The predict method allows us to get the predictions of our model on a whole dataset. We can the use those predictions to compute metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"auto\">4. Ù…ØªØ¯ predict Ø¨Ù‡ Ù…Ø§ Ø§ÛŒÙ† Ø§Ù…Ú©Ø§Ù† Ø±Ø§ Ù…ÛŒ Ø¯Ù‡Ø¯ Ú©Ù‡ predictÙ‡Ø§ÛŒ Ù…Ø¯Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¯Ø³Øª Ø¢ÙˆØ±ÛŒÙ…. Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² Ø§ÛŒÙ† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ (Ø³Ù†Ø¬Ø´â€ŒÙ‡Ø§) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ….</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
